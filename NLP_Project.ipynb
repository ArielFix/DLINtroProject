{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArielFix/DLINtroProject/blob/ImportRobertaPreTrained/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "gXXJiRFgYhKz",
        "outputId": "c0e7f70e-9caa-433e-e11b-cceec1747099"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-296801345cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbranch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"git\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rev-parse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--abbrev-ref\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<a href=\"https://colab.research.google.com/github/ArielFix/DLINtroProject/blob/{branch}/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    416\u001b[0m                **kwargs).stdout\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'rev-parse', '--abbrev-ref', 'HEAD']' returned non-zero exit status 128."
          ]
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "import subprocess\n",
        "\n",
        "branch = subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"]).decode()[:-1]\n",
        "HTML('<a href=\"https://colab.research.google.com/github/ArielFix/DLINtroProject/blob/{branch}/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'.format(branch=branch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpMqyzzkYhLB"
      },
      "source": [
        "## Notes:\n",
        "* Running may require commands in the bottom of the notebook first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "peWUcuINV7sS"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "804WKMCAWEe7",
        "outputId": "2958faf2-bcd7-4a1d-f39c-f06bee7ef057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "train_path = '/content/gdrive/MyDrive/NLP_Project/data/train.csv'\n",
        "test_path = '/content/gdrive/My Drive/NLP_Project/data/test.csv'\n",
        "sample_submission_path = '/content/gdrive/MyDrive/NLP_Project/data/sample_submission.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yAnJ7d3vYqsV"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "#print(train_df.to_string())\n",
        "#print(test_df.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0syMlj9ae_E"
      },
      "source": [
        "**A quick look at our data**\n",
        "Let's look at our data... first, some examples of what is NOT a disaster tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byonChzkajoQ",
        "outputId": "aa572b5d-476e-4d25-ecbc-e29049c89edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What's up man?\n",
            "I love fruits\n",
            "Summer is lovely\n",
            "My car is so fast\n",
            "What a goooooooaaaaaal!!!!!!\n"
          ]
        }
      ],
      "source": [
        "print('\\n' .join(map(str, train_df[train_df[\"target\"] == 0][\"text\"].values[0:5])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM_OM_EScYgJ"
      },
      "source": [
        "And some examples of what is a disaster tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_DdvOhrcc8s",
        "outputId": "7484d044-2143-4039-e237-a85a64e2f6ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
            "Forest fire near La Ronge Sask. Canada\n",
            "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
            "13,000 people receive #wildfires evacuation orders in California \n",
            "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n"
          ]
        }
      ],
      "source": [
        "print('\\n' .join(map(str, train_df[train_df[\"target\"] == 1][\"text\"].values[0:5])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5_Jex7SdJdU"
      },
      "source": [
        "The words contained in each tweet are a good indicator of whether they're about a real disaster or not. This is not entirely correct, but it's a great place to start.\n",
        "\n",
        "We'll use scikit-learn's `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLFGPgObdhfi",
        "outputId": "45501f90-91a1-4bf1-eb49-67dc8e8ece39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 5 tweets in the data: \n",
            "\n",
            "shawie17shawie: Breaking news! Unconfirmed! I just heard a loud bang nearby. in what appears to be a blast of wind from my neighbour's ass.\n",
            "Fotoset: elanorofrohan: 10th December 2013 Green Carpet in Zurich for the Swiss Premiere of The Desolation... http://t.co/BQ3P7n7w06\n",
            "Ngata on injury list at start of practice for Lions http://t.co/Z16DtoQHhG\n",
            "That sounds like a really bad idea I like Yoenis but I feel like his production could fall off a huge cliff.\n",
            "...the kids at the orphanage were obviously not too traumatised. http://t.co/DjA4relcnS\n",
            "\n",
            "\n",
            "(1, 70)\n",
            "[[0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0\n",
            "  0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "train_df, val_df = model_selection.train_test_split(train_df)\n",
        "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
        "\n",
        "## let's get counts for the first 5 tweets in the data\n",
        "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])\n",
        "print('The first 5 tweets in the data: \\n')\n",
        "print('\\n' .join(map(str, train_df[\"text\"][0:5])))\n",
        "print('\\n')\n",
        "## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
        "print(example_train_vectors[0].todense().shape)\n",
        "print(example_train_vectors[0].todense())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usIeYe9Qe-jg"
      },
      "source": [
        "**The above tells us that:**\n",
        "\n",
        "1.   There are 54 unique words (or \"tokens\") in the first five tweets.\n",
        "2.   The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcA5TFkdfjHZ"
      },
      "source": [
        "Now let's create vectors for all of our tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OtIFyab_fit-"
      },
      "outputs": [],
      "source": [
        "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
        "\n",
        "## note that we're NOT using .fit_transform() on test_df. Using just .transform() makes sure\n",
        "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
        "# i.e. that the train and test vectors use the same set of tokens.\n",
        "val_vectors = count_vectorizer.transform(val_df[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpPhYkkGf_jl"
      },
      "source": [
        "**Our model**\n",
        "\n",
        "As we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n",
        "\n",
        "What we're assuming here is a linear connection. So let's build a linear model and see!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VRwkRi7hgBaA"
      },
      "outputs": [],
      "source": [
        "## Our vectors are really big, so we want to push our model's weights\n",
        "## toward 0 without completely discounting different words - ridge regression \n",
        "## is a good way to do this.\n",
        "clf = linear_model.RidgeClassifier()\n",
        "clf.fit(train_vectors, train_df[\"target\"]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqW_HlFzLYUd",
        "outputId": "4baabb62-50e9-4d4c-ff07-c8156e6ff137"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7117117117117117"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "val_target = clf.predict(val_vectors)\n",
        "f1_score(y_pred=val_target, y_true=val_df[\"target\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erAJlNvqKglh"
      },
      "source": [
        "Let's test our model and see how well it does on the training data. For this we'll use cross-validation - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n",
        "\n",
        "The metric for this competition is F1, so let's use that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug_ecfZuKxxE",
        "outputId": "69e4a174-49f6-4a09-fbf2-b842fa495b66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.71250819, 0.71848465, 0.72293814])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "3522czRaPt5B"
      },
      "outputs": [],
      "source": [
        "results = pd.read_csv(sample_submission_path)\n",
        "results.target = clf.predict(count_vectorizer.transform(test_df[\"text\"]))\n",
        "results.to_csv(\"submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFJs2yb-PqnQ"
      },
      "source": [
        "![ridge_score.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I42bT7YlK1Q1"
      },
      "source": [
        "The above scores aren't terrible! It looks like our assumption will score roughly 0.65 on the leaderboard. There are lots of ways to potentially improve on this (TFIDF, LSA, LSTM / RNNs, the list is long!) - we will give any of them a shot!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcux5Ux2PP9y"
      },
      "source": [
        "\n",
        "Here's how we could use an RNN to solve the disaster tweets problem:\n",
        "\n",
        "\n",
        "1.   Preprocess the tweet data by tokenizing the text and creating a vocabulary. we \n",
        "may also want to consider applying techniques such as stemming and lemmatization to help reduce the dimensionality of the data.\n",
        "3. Pad the sequences of integers to ensure that all of the tweets have the same length.\n",
        "4. Build an RNN model with an embedding layer, followed by one or more LSTM or GRU layers, and ending with a dense layer for classification.\n",
        "5. Train the model on the disaster tweets data, using binary cross-entropy loss and an optimizer such as Adam.\n",
        "6. Evaluate the model's performance on a held-out test set and fine-tune the model as needed to improve its accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_4anoqqQXpN"
      },
      "source": [
        "We'll may also try and fine-tune: [twitter-roberta-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) for our purposes. If possible within the given schedule and resources. We will also see if we the results from the pre-trained model improve our model performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11xfcYs0ajbk",
        "outputId": "9d8ed808-af2f-41d7-9200-2171d2d820ff"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "OpBN3F1-agUI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imperting pre-trained roberta:\n",
        "task='sentiment'\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "yhMjkW1fa7v5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing roberta model architecture\n",
        "roberta_model.modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne8Bz-j_cpaE",
        "outputId": "cecf20d3-7fe8-418a-ec07-d7f5c279e256"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing roberta model last layer to adjust the output to the required task\n",
        "\n",
        "roberta_model.classifier.out_proj = nn.Sequential(nn.Linear(in_features=768, out_features=1, bias=True), nn.Sigmoid())\n",
        "\n",
        "roberta_model.modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG1SoXnwcYN9",
        "outputId": "da02fcdf-5155-4863-efb6-d1c5a0a39dc1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Sequential(\n",
              "      (0): Linear(in_features=768, out_features=1, bias=True)\n",
              "      (1): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing roberta model parameters - only the classifier can be trained\n",
        "for parameter in roberta_model.parameters():\n",
        "  parameter.requires_grad = False\n",
        "\n",
        "for classifier_parameter in roberta_model.classifier.parameters():\n",
        "  classifier_parameter.requires_grad = True\n",
        "\n",
        "# for parameter in roberta_model.parameters():\n",
        "#   print(parameter.requires_grad)"
      ],
      "metadata": {
        "id": "KQvRopJXghhJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, val_text = model_selection.train_test_split(train_df[\"text\"])\n",
        "print(\"train text example:\")\n",
        "for text in train_text[0:2]:\n",
        "  print(text)\n",
        "\n",
        "print(\"val text example:\")\n",
        "for text in val_text[0:2]:\n",
        "  print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjwCsHVbpeco",
        "outputId": "22c779f7-3a4d-4feb-d9f3-107c12154115"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train text example:\n",
            "@RachelRofe tired it' 5:36 am. Woke up to a thunderstorm lightning and rain. How are you?\n",
            "#4: The Hobbit: The Desolation of Smaug (Bilingual) http://t.co/G5dO2X6226\n",
            "val text example:\n",
            "Deepak Chopra's EPIC Twitter Meltdown http://t.co/ethgAGPy5G\n",
            "My brother is crying cause the thunder lmao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_train = tokenizer(train_text.to_list()[0:30], return_tensors='pt', padding=True)\n",
        "preprocessed_val = tokenizer(val_text.to_list(), return_tensors='pt', padding=True)"
      ],
      "metadata": {
        "id": "Pz-1V5JUAUTA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_text.to_list()[0:10], batch_size=30, shuffle=True, num_workers=2) \n",
        "val_loader = torch.utils.data.DataLoader(train_text.to_list(), batch_size=10, shuffle=True, num_workers=2) \n",
        "\n",
        "for _, batch in enumerate(train_loader):\n",
        "  data = batch\n",
        "  v = roberta_model(**tokenizer(data, return_tensors='pt', padding=True))\n",
        "  print(v.logits.detach().numpy())"
      ],
      "metadata": {
        "id": "7jSakPyNCgzq",
        "outputId": "11ff7b7b-6697-4668-9734-d25aefac7088",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.52495193]\n",
            " [0.53614074]\n",
            " [0.50322306]\n",
            " [0.48913515]\n",
            " [0.4908721 ]\n",
            " [0.5058625 ]\n",
            " [0.51355755]\n",
            " [0.5344084 ]\n",
            " [0.54377913]\n",
            " [0.5094015 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v.logits.detach()"
      ],
      "metadata": {
        "id": "DTgWoBolQ43h",
        "outputId": "09d60a41-ede9-402d-e5fd-4b7fae17c63f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5361],\n",
              "        [0.5250],\n",
              "        [0.5438],\n",
              "        [0.4909],\n",
              "        [0.5094],\n",
              "        [0.4891],\n",
              "        [0.5344],\n",
              "        [0.5136],\n",
              "        [0.5032],\n",
              "        [0.5059]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_roberta_model(roberta_model, learning_rate, weight_decay, train_data, val_data, batch_size, num_epoches, check_point_path):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          weight_decay=weight_decay)\n",
        "\n",
        "  iters, losses = [], []\n",
        "  iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "  \n",
        "  for epoch in range(0,num_epoches):\n",
        "    \n"
      ],
      "metadata": {
        "id": "kk2y4gZ8rg-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PT\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "model.save_pretrained(MODEL)\n",
        "\n",
        "text = \"Good night 😊\"\n",
        "text = preprocess(text)\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "scores = output[0][0].detach().numpy()\n",
        "scores = softmax(scores)\n",
        "\n",
        "# # TF\n",
        "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "# model.save_pretrained(MODEL)\n",
        "\n",
        "# text = \"Good night 😊\"\n",
        "# encoded_input = tokenizer(text, return_tensors='tf')\n",
        "# output = model(encoded_input)\n",
        "# scores = output[0][0].numpy()\n",
        "# scores = softmax(scores)\n",
        "\n",
        "ranking = np.argsort(scores)\n",
        "ranking = ranking[::-1]\n",
        "for i in range(scores.shape[0]):\n",
        "    l = labels[ranking[i]]\n",
        "    s = scores[ranking[i]]\n",
        "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "ETexq_usZRJJ",
        "outputId": "02547c8e-e90a-418f-ceee-61a56153ee0d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-33b2e703bd60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# PT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Good night 😊\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_pooling_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaClassificationHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaPooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madd_pooling_layer\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRobertaLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRobertaLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self} should be used as a decoder model if cross attention is added\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_embedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaIntermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACT2FN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_act\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGiEitqjYhMM"
      },
      "source": [
        "## Setup Commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaXEQzUiYhMN",
        "outputId": "04bc6652-5a04-410f-e89e-5a6718bdd129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in e:/Python Scripts/bci_mouse/DLINtroProject/test/.git/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: remote origin already exists.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Switched to a new branch 'main'\n"
          ]
        }
      ],
      "source": [
        "!git init .\n",
        "!git remote add origin https://github.com/ArielFix/DLINtroProject.git\n",
        "!git fetch\n",
        "!git checkout main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMCOjNZtYhMO",
        "outputId": "6843c4b3-0f5c-40c9-fe82-1d8a4e5914f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"This may take a while:\"\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
            "You should consider upgrading via the 'e:\\Python Scripts\\bci_mouse\\DLINtroProject\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!echo \"This may take a while:\"\n",
        "%pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gck5OX6YhMP"
      },
      "source": [
        "## Troubleshooting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBDeFVQvYhMP",
        "outputId": "a4f92c2c-50c7-49e0-eb50-0fecb2cc84bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synonym Replace: hullo @world, let's see if the #augmentation go! GO! GO! GO!\n",
            "Back Translate: Hello @world, let's see if the #Augmentation work! WALK! WALK! WALK!\n",
            "Random Delete: Hello @world, 's  if  #augmentations work! GO! GO! GO!\n",
            "Random Insertion: Hello @ world, let's see if the #augmentation augmentations work! GO hello! GO! GO!\n",
            "Remove Duplicates: Hello @ world, let's see if the #augmentations work! GO\n",
            "Swap Words: 's @ GO #let Hello see, the if augmentations work GO GO! world!!!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using state Tel Aviv server backend.\n",
            "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shaiel\n",
            "[nltk_data]     Cohen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\Shaiel\n",
            "[nltk_data]     Cohen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Shaiel\n",
            "[nltk_data]     Cohen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!python test_augs.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25-VVQD8YhMQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.0 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "7645dde2e29746a0cd35ce904051cb398f45ecdefb65fe0e340e36ef16380419"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}